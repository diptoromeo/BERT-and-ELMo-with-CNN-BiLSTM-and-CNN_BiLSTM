{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYaIe5lqvSTsDsvS6yBE/N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diptoromeo/BERT-and-ELMo-with-CNN-BiLSTM-and-CNN_BiLSTM/blob/main/ELMo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs4ppcfFiVgs",
        "outputId": "d8e68af6-0e2b-458b-c4c6-5fc7e0eb690e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp\n",
            "  Downloading allennlp-2.10.1-py3-none-any.whl (730 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.2/730.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<1.13.0,>=1.10.0 (from allennlp)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n",
            "  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl (19.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.1/19.1 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n",
            "  Downloading cached_path-1.1.6-py3-none-any.whl (26 kB)\n",
            "Collecting fairscale==0.4.6 (from allennlp)\n",
            "  Downloading fairscale-0.4.6.tar.gz (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.8.1)\n",
            "Collecting spacy<3.4,>=2.1.0 (from allennlp)\n",
            "  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.4 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.23.5)\n",
            "Collecting tensorboardX>=1.2 (from allennlp)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from allennlp) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.10/dist-packages (from allennlp) (4.66.1)\n",
            "Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.11.4)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (7.4.3)\n",
            "Collecting transformers<4.21,>=4.1 (from allennlp)\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece>=0.1.96 (from allennlp)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock<3.8,>=3.3 (from allennlp)\n",
            "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
            "Collecting lmdb>=1.2.1 (from allennlp)\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools>=8.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (10.1.0)\n",
            "Collecting termcolor==1.1.0 (from allennlp)\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.19.4)\n",
            "Collecting dill>=0.3.4 (from allennlp)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting base58>=2.1.1 (from allennlp)\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting sacremoses (from allennlp)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.9.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.20.3)\n",
            "Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n",
            "Collecting jsonnet>=0.10.0 (from allennlp)\n",
            "  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading boto3-1.34.11-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n",
            "Collecting huggingface-hub>=0.0.16 (from allennlp)\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.0.16->allennlp) (23.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->allennlp) (2023.6.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=6.2.5->allennlp) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->allennlp) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.1->allennlp) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.0.9)\n",
            "Collecting thinc<8.1.0,>=8.0.14 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.7.11)\n",
            "Collecting wasabi<1.1.0,>=0.9.1 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (2.0.10)\n",
            "Collecting typer>=0.4.1 (from allennlp)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (6.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from spacy<3.4,>=2.1.0->allennlp)\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.4,>=2.1.0->allennlp) (3.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision<0.14.0,>=0.8.1->allennlp) (9.4.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers<4.21,>=4.1->allennlp)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from wandb<0.13.0,>=0.10.0->allennlp) (1.16.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting botocore<1.35.0,>=1.34.11 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading botocore-1.34.11-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.17.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.7.0)\n",
            "Collecting commonmark<0.10.0,>=0.9.0 (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp)\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0,>=12.1->cached-path<1.2.0,>=1.1.3->allennlp) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.4,>=2.1.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.11->boto3<2.0,>=1.0->cached-path<1.2.0,>=1.1.3->allennlp) (2.8.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb<0.13.0,>=0.10.0->allennlp)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.62.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (1.5.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path<1.2.0,>=1.1.3->allennlp) (0.5.1)\n",
            "Building wheels for collected packages: fairscale, termcolor, jsonnet, pathtools\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307222 sha256=c16815dc91d7907a655e8dff234e704f4c68fef06ab524dc22e8059e8c41f9cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/58/3d/e114952ab4a8f31eb9dae230658450afff986b211a5b1f2256\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=74d30c97c03ec5c7ab4e46423edffa9e73c043af5988c4426f029cd588845734\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.20.0-cp310-cp310-linux_x86_64.whl size=6406862 sha256=89bce96c5e16dd65ac82014ae23637d62d9bac7e40702a2d979a204b602e8550\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/0d/6b/5467dd1db9332ba4bd5cf4153e2870c5f89bb4db473d989cc2\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=ef5502aebef606d2890b4851be1a9497621762a2efef99baae4ca8aa61836ae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built fairscale termcolor jsonnet pathtools\n",
            "Installing collected packages: wasabi, tokenizers, termcolor, sentencepiece, pathtools, lmdb, jsonnet, commonmark, typer, torch, tensorboardX, smmap, shortuuid, setproctitle, sentry-sdk, sacremoses, rich, pydantic, jmespath, filelock, docker-pycreds, dill, base58, torchvision, thinc, huggingface-hub, gitdb, fairscale, botocore, transformers, spacy, s3transfer, GitPython, wandb, boto3, cached-path, allennlp\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.2\n",
            "    Uninstalling wasabi-1.1.2:\n",
            "      Successfully uninstalled wasabi-1.1.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.4.0\n",
            "    Uninstalling termcolor-2.4.0:\n",
            "      Successfully uninstalled termcolor-2.4.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.0\n",
            "    Uninstalling typer-0.9.0:\n",
            "      Successfully uninstalled typer-0.9.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.7.0\n",
            "    Uninstalling rich-13.7.0:\n",
            "      Successfully uninstalled rich-13.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.12\n",
            "    Uninstalling thinc-8.1.12:\n",
            "      Successfully uninstalled thinc-8.1.12\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.6.1\n",
            "    Uninstalling spacy-3.6.1:\n",
            "      Successfully uninstalled spacy-3.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "en-core-web-sm 3.6.0 requires spacy<3.7.0,>=3.6.0, but you have spacy 3.3.3 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 allennlp-2.10.1 base58-2.1.1 boto3-1.34.11 botocore-1.34.11 cached-path-1.1.6 commonmark-0.9.1 dill-0.3.7 docker-pycreds-0.4.0 fairscale-0.4.6 filelock-3.7.1 gitdb-4.0.11 huggingface-hub-0.10.1 jmespath-1.0.1 jsonnet-0.20.0 lmdb-1.4.1 pathtools-0.1.2 pydantic-1.8.2 rich-12.6.0 s3transfer-0.10.0 sacremoses-0.1.1 sentencepiece-0.1.99 sentry-sdk-1.39.1 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 spacy-3.3.3 tensorboardX-2.6.2.2 termcolor-1.1.0 thinc-8.0.17 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.20.1 typer-0.4.2 wandb-0.12.21 wasabi-0.10.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLCJ10p0RBHo",
        "outputId": "bb593edc-01b9-427e-96d4-1e32f6fd92e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 14005374764160712546\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14626652160\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 7722961162730036849\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import keras.models\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "import os\n",
        "import pandas as pd\n",
        "import regex\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Lambda, Input, TimeDistributed, LSTM, Bidirectional\n",
        "from keras.layers import Conv1D, Flatten, Dropout, GlobalMaxPool1D, MaxPooling1D\n",
        "#from tensorflow.keras.layers import Conv1D, Flatten, Dropout, GlobalMaxPool1D, MaxPooling1D, Dense\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers, models, Sequential\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.python.client import device_lib\n",
        "import timeit\n",
        "tf.compat.v1.experimental.output_all_intermediates(True) #Graph execution for BiLSTM Models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Embedding, Dropout\n",
        "print(tf.__version__)\n",
        "print(device_lib.list_local_devices())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heading 1\n",
        "Preprocessing the Dataset"
      ],
      "metadata": {
        "id": "u1Op2nnpvsyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##===============================Nltk abstract_words tokenize======================================\n",
        "with open('/content/FGFSJournal.txt', 'rt', encoding='UTF8') as file:\n",
        "    FGCS_abstract = []\n",
        "    for line in file:\n",
        "        if '<abstract>' in line:\n",
        "            abstract = line.split('</abstract>')[0].split('<abstract>')[-1]\n",
        "            abstract = ''.join(i for i in abstract if not i.isdigit())\n",
        "            abstract = regex.sub('[^\\w\\d\\s]+', '', abstract)\n",
        "            ##abstract = nltk.sent_tokenize(abstract)\n",
        "            abstract = nltk.word_tokenize(abstract)\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            filtered_sentence_abstract = [w.lower() for w in abstract if\n",
        "                                          w.lower() not in punctuation and w.lower() not in stop_words]\n",
        "            tagged_list = nltk.pos_tag(filtered_sentence_abstract)\n",
        "            nouns_list = [t[0] for t in tagged_list if t[-1] == 'NN']\n",
        "            lm = WordNetLemmatizer()\n",
        "            singluar_form = [lm.lemmatize(w, pos='v') for w in nouns_list]\n",
        "            FGCS_abstract.append(singluar_form)\n",
        "\n",
        "print(len(FGCS_abstract))\n",
        "print(FGCS_abstract[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDZ5C_5JT7bJ",
        "outputId": "b27b3d59-bb87-49f5-8ca0-b6e6537ab310"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5659\n",
            "['architecture', 'inference']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Heading 2\n",
        "Spling the preprocessing dataset to Train and Test data."
      ],
      "metadata": {
        "id": "pS9r69zwv01l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(FGCS_abstract)\n",
        "train_data = FGCS_abstract[:4527]\n",
        "test_data = FGCS_abstract[4527:]\n",
        "print(\"train data:\", len(train_data))\n",
        "print(\"test data:\", len(test_data))\n",
        "\n",
        "train, valid = train_test_split(train_data, test_size=0.25, random_state=42)\n",
        "print(\"train:\", len(train))\n",
        "print(\"valid:\", len(valid))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFyLex7LVgvg",
        "outputId": "c9319f78-d925-4c2b-e85d-66ad42711265"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train data: 4527\n",
            "test data: 1132\n",
            "train: 3395\n",
            "valid: 1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Labeling\n",
        "Making Data Labeling about Top-10, 20, 30 words"
      ],
      "metadata": {
        "id": "4tsGt-McvhPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#======================================================train_labels==========================================================================\n",
        "ten_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud']\n",
        "twenty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource']\n",
        "thirty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource',\n",
        "               'environment', 'algorithm', 'energy', 'management', 'architecture', 'access', 'scheme', 'communication', 'execution', 'order']\n",
        "\n",
        "\n",
        "\n",
        "##==============================10-words label==================================\n",
        "train10_labels = []\n",
        "\n",
        "for doc in train:\n",
        "    label = []\n",
        "    for term in ten_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    train10_labels.append(label)\n",
        "\n",
        "print(\"10_train labels:\", len(train10_labels))\n",
        "\n",
        "##==================valid_labels==================\n",
        "valid10_labels = []\n",
        "for doc in valid:\n",
        "    label = []\n",
        "    for term in ten_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    valid10_labels.append(label)\n",
        "\n",
        "print(\"10_valid labels:\", len(valid10_labels))\n",
        "\n",
        "##==================Test_labels==================\n",
        "test10_labels = []\n",
        "for doc in test_data:\n",
        "    label = []\n",
        "    for term in ten_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    test10_labels.append(label)\n",
        "\n",
        "print(\"10_test labels:\", len(test10_labels))\n",
        "\n",
        "\n",
        "##==============================20-words label==================================\n",
        "train20_labels = []\n",
        "for doc in train:\n",
        "    label = []\n",
        "    for term in twenty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    train20_labels.append(label)\n",
        "\n",
        "print(\"20_train labels:\", len(train20_labels))\n",
        "\n",
        "\n",
        "##==================valid_labels==================\n",
        "valid20_labels = []\n",
        "for doc in valid:\n",
        "    label = []\n",
        "    for term in twenty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    valid20_labels.append(label)\n",
        "\n",
        "print(\"20_valid labels:\", len(valid20_labels))\n",
        "\n",
        "\n",
        "##==================Test_labels==================\n",
        "test20_labels = []\n",
        "for doc in test_data:\n",
        "    label = []\n",
        "    for term in twenty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    test20_labels.append(label)\n",
        "\n",
        "print(\"20_test labels:\", len(test20_labels))\n",
        "\n",
        "\n",
        "##==============================30-words label==================================\n",
        "train30_labels = []\n",
        "for doc in train:\n",
        "    label = []\n",
        "    for term in thirty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    train30_labels.append(label)\n",
        "\n",
        "print(\"30_train labels:\", len(train30_labels))\n",
        "\n",
        "\n",
        "##==================valid_labels==================\n",
        "valid30_labels = []\n",
        "for doc in valid:\n",
        "    label = []\n",
        "    for term in thirty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    valid30_labels.append(label)\n",
        "\n",
        "print(\"30_valid labels:\", len(valid30_labels))\n",
        "\n",
        "\n",
        "##==================Test_labels==================\n",
        "test30_labels = []\n",
        "for doc in test_data:\n",
        "    label = []\n",
        "    for term in thirty_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    test30_labels.append(label)\n",
        "\n",
        "print(\"30_test labels:\", len(test30_labels))\n"
      ],
      "metadata": {
        "id": "QOIGMnKnVs--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706ff914-1bec-464d-bb82-e93b82e0bac7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10_train labels: 3395\n",
            "10_valid labels: 1132\n",
            "10_test labels: 1132\n",
            "20_train labels: 3395\n",
            "20_valid labels: 1132\n",
            "20_test labels: 1132\n",
            "30_train labels: 3395\n",
            "30_valid labels: 1132\n",
            "30_test labels: 1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELMo Embeddings"
      ],
      "metadata": {
        "id": "dy4eYdQ1VFde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")"
      ],
      "metadata": {
        "id": "ZW5a0j2uAjQh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Representatiom"
      ],
      "metadata": {
        "id": "jSAbG8xnVM60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize and pad sequences\n",
        "max_words = 100001\n",
        "max_sequence_length = 156\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "sequences = tokenizer.texts_to_sequences(train_data)\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "tokenizer.fit_on_texts(test_data)\n",
        "test = tokenizer.texts_to_sequences(test_data)\n",
        "X_test = pad_sequences(test, maxlen=max_sequence_length)\n",
        "\n",
        "y10_train = np.array(train10_labels)\n",
        "y20_train = np.array(train20_labels)\n",
        "y30_train = np.array(train30_labels)\n",
        "y10_valid = np.array(valid10_labels)\n",
        "y20_valid = np.array(valid20_labels)\n",
        "y30_valid = np.array(valid30_labels)\n",
        "y10_test = np.array(test10_labels)\n",
        "y20_test = np.array(test20_labels)\n",
        "y30_test = np.array(test30_labels)\n",
        "\n",
        "y10_test = np.argmax(y10_test, axis=1)\n",
        "y20_test = np.argmax(y20_test, axis=1)\n",
        "y30_test = np.argmax(y30_test, axis=1)\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_valid = train_test_split(X,  test_size=0.25, random_state=42)\n",
        "\n",
        "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
        "embedding_dim = 1024  # ELMo embeddings have a dimension of 1024\n",
        "\n",
        "# Add an Embedding layer to the model\n",
        "vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
        "embedding_dim = 1024  # This should match the ELMo embedding dimension"
      ],
      "metadata": {
        "id": "O4K9G8ZZVRY-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Models Hyperparameters"
      ],
      "metadata": {
        "id": "HagGOLMk471y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_sizes = 16\n",
        "bilstm_embedding_dim = 256\n",
        "filter_sizes = 128\n",
        "kernel_sizes = 5\n",
        "hidden_layers = 64\n",
        "lstm_memory = 64\n",
        "bilstm_memory = 50\n",
        "dropout = 0.5\n",
        "ten_num_classes = 10\n",
        "twenty_num_classes = 20\n",
        "thirty_num_classes = 30"
      ],
      "metadata": {
        "id": "bzBfV_Q_46JT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELMo + CNN Model"
      ],
      "metadata": {
        "id": "E73LNHHQFqxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELMo Embedding\n",
        "Making ELMo Embedding to get ELMo from TensoFlow"
      ],
      "metadata": {
        "id": "akpAjQEAvJyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_eager_execution()"
      ],
      "metadata": {
        "id": "jizmvnqftG0E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ten_word_cnn_model():\n",
        "  cnn_model = Sequential()\n",
        "  cnn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "  cnn_model.add(Conv1D(filters=filter_sizes, kernel_size=kernel_sizes, activation='relu'))\n",
        "  cnn_model.add(GlobalMaxPooling1D())\n",
        "  cnn_model.add(Dense(hidden_layers, activation='relu'))\n",
        "  cnn_model.add(Dropout(dropout))\n",
        "  cnn_model.add(Dense(ten_num_classes, activation='softmax'))\n",
        "  cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  cnn_model.summary()\n",
        "  return cnn_model\n",
        "\n",
        "\n",
        "def twenty_word_cnn_model():\n",
        "  cnn_model = Sequential()\n",
        "  cnn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "  cnn_model.add(Conv1D(filters=filter_sizes, kernel_size=kernel_sizes, activation='relu'))\n",
        "  cnn_model.add(GlobalMaxPooling1D())\n",
        "  cnn_model.add(Dense(hidden_layers, activation='relu'))\n",
        "  cnn_model.add(Dropout(dropout))\n",
        "  cnn_model.add(Dense(twenty_num_classes, activation='softmax'))\n",
        "  cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  cnn_model.summary()\n",
        "  return cnn_model\n",
        "\n",
        "\n",
        "def thirty_word_cnn_model():\n",
        "  cnn_model = Sequential()\n",
        "  cnn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "  cnn_model.add(Conv1D(filters=filter_sizes, kernel_size=kernel_sizes, activation='relu'))\n",
        "  cnn_model.add(GlobalMaxPooling1D())\n",
        "  cnn_model.add(Dense(hidden_layers, activation='relu'))\n",
        "  cnn_model.add(Dropout(dropout))\n",
        "  cnn_model.add(Dense(thirty_num_classes, activation='softmax'))\n",
        "  cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  cnn_model.summary()\n",
        "  return cnn_model\n",
        "\n",
        "\n",
        "cnn_model1 = ten_word_cnn_model()\n",
        "cnn_model2 = twenty_word_cnn_model()\n",
        "cnn_model3 = thirty_word_cnn_model()\n",
        "\n",
        "cnn_model1.save(\"ten_word_cnn_model\")\n",
        "cnn_model1.save(\"twenty_word_cnn_model\")\n",
        "cnn_model1.save(\"thirty_word_cnn_model\")\n",
        "\n",
        "\n",
        "##=================10words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history10 = cnn_model1.fit(X_train, y10_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo10_CNN_Accuracy = cnn_model1.evaluate(X_train, y10_train, verbose=1)\n",
        "    print(('ELMo10_CNN_Accuracy: %f' % (ELMo10_CNN_Accuracy[1] * 100)))\n",
        "\n",
        "\n",
        "##===============================ELMo10_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = cnn_model1.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo10_CNN_Test_Accuracy = accuracy_score(y10_test, np.round(abs(predictions)))\n",
        "    print('ELMo10_CNN_Test_Accuracy: %f' % ELMo10_CNN_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo10_CNN_Precision = precision_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_Precision: %f' % ELMo10_CNN_Precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo10_CNN_Recall = recall_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_Recall: %f' % ELMo10_CNN_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo10_CNN_F1_Score = f1_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_F1_Score: %f' % ELMo10_CNN_F1_Score)\n",
        "\n",
        "\n",
        "cnn_data10 = {\n",
        "    'Name': [\"ELMo10_CNN_Accuracy\", \"ELMo10CNN_Test_Accuracy\", \"ELMo10_CNN_Precision\", \"ELMo10_CNN_Recall\", \"ELMo10_CNN_F1_Score\"],\n",
        "    'Scores': [ELMo10_CNN_Accuracy, ELMo10_CNN_Test_Accuracy, ELMo10_CNN_Precision, ELMo10_CNN_Recall, ELMo10_CNN_F1_Score] #\n",
        "}\n",
        "df = pd.DataFrame(cnn_data10)\n",
        "df.to_excel('ELMo10_CNN_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "##=================20words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history20 = cnn_model2.fit(X_train, y20_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo20_CNN_Accuracy = cnn_model2.evaluate(X_train, y20_train, verbose=1)\n",
        "    print(('ELMo20_CNN_Accuracy: %f' % (ELMo20_CNN_Accuracy[1] * 100)))\n",
        "\n",
        "##===============================ELMo20_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = cnn_model2.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo20_CNN_Test_Accuracy = accuracy_score(y20_test, np.round(abs(predictions)))\n",
        "    print('ELMo20_CNN_Test_Accuracy y: %f' % ELMo20_CNN_Test_Accuracy )\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo20_CNN_Precision = precision_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_Precision: %f' % ELMo20_CNN_Precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo20_CNN_Recall = recall_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_Recall: %f' % ELMo20_CNN_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo20_CNN_F1_Score = f1_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_F1_Score: %f' % ELMo20_CNN_F1_Score)\n",
        "\n",
        "\n",
        "cnn_data20 = {\n",
        "    'Name': [\"ELMo20_CNN_Accuracy\", \"ELMo20_CNN_Test_Accuracy \", \"ELMo20CNN_Precision\", \"ELMo20_CNN_Recall\", \"ELMo20_CNN_F1_Score\"],\n",
        "    'Scores': [ELMo20_CNN_Accuracy, ELMo20_CNN_Test_Accuracy , ELMo20_CNN_Precision, ELMo20_CNN_Recall, ELMo20_CNN_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(cnn_data20)\n",
        "df.to_excel('ELMo20_CNN_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "##=================30words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history30 = cnn_model3.fit(X_train, y30_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo30_CNN_Accuracy = cnn_model1.evaluate(X_train, y10_train, verbose=1)\n",
        "    print(('ELMo30_CNN_Accuracy: %f' % (ELMo30_CNN_Accuracy[1] * 100)))\n",
        "\n",
        "##===============================ELMo30_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = cnn_model3.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo30_CNN_Test_Accuracy = accuracy_score(y30_test, np.round(abs(predictions)))\n",
        "    print('ELMo30_CNN_Test_Accuracy: %f' % ELMo30_CNN_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo30_CNN_Precision = precision_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_Precision: %f' % ELMo30_CNN_Precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo30_CNN_Recall = recall_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_Recall: %f' % ELMo30_CNN_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo30_CNN_F1_Score = f1_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_F1_Score: %f' % ELMo30_CNN_F1_Score)\n",
        "\n",
        "\n",
        "cnn_data30 = {\n",
        "    'Name': [\"ELMo30_CNN_F1_Score\", \"ELMo30_CNN_Test_Accuracy\", \"ELMo30_CNN_Precision\", \"ELMo30_CNN_Recall\", \"ELMo30_CNN_F1_Score\"],\n",
        "    'Scores': [ELMo30_CNN_F1_Score, ELMo30_CNN_Test_Accuracy, ELMo30_CNN_Precision, ELMo30_CNN_Recall, ELMo30_CNN_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(cnn_data30)\n",
        "df.to_excel('ELMo30_CNN_Scores_file.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxhXgGksbqYl",
        "outputId": "3b1d8c2a-3bab-46d5-e8f5-68bc53bdcc33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 156, 1024)         13281280  \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 152, 128)          655488    \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13945674 (53.20 MB)\n",
            "Trainable params: 13945674 (53.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 156, 1024)         13281280  \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 152, 128)          655488    \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 20)                1300      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13946324 (53.20 MB)\n",
            "Trainable params: 13946324 (53.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 156, 1024)         13281280  \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 152, 128)          655488    \n",
            "                                                                 \n",
            " global_max_pooling1d_5 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 30)                1950      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13946974 (53.20 MB)\n",
            "Trainable params: 13946974 (53.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 24s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.6186 - accuracy: 0.7096\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 9s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.5209 - accuracy: 0.7213\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 9s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.4205 - accuracy: 0.7381\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 9s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.3236 - accuracy: 0.7656\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 9s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.2538 - accuracy: 0.7799\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.2048 - accuracy: 0.7862\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1723 - accuracy: 0.7894\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1508 - accuracy: 0.7907\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1325 - accuracy: 0.7927\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1202 - accuracy: 0.7934\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1123 - accuracy: 0.7941\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1062 - accuracy: 0.7942\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.1016 - accuracy: 0.7943\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0994 - accuracy: 0.7947\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0964 - accuracy: 0.7946\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0925 - accuracy: 0.7953\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0893 - accuracy: 0.7957\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0889 - accuracy: 0.7955\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0865 - accuracy: 0.7959\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 10s 1s/step - batch: 3.5000 - size: 1.0000 - loss: 0.0840 - accuracy: 0.7956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_CNN_Accuracy: 78.114891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_CNN_Test_Accuracy: 0.678445\n",
            "ELMo10_CNN_Precision: 0.678445\n",
            "ELMo10_CNN_Recall: 0.678445\n",
            "ELMo10_CNN_F1_Score: 0.678445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 6s 587ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6589 - accuracy: 0.7690\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 5s 591ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5999 - accuracy: 0.7691\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 5s 594ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5377 - accuracy: 0.7696\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 5s 598ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4738 - accuracy: 0.7731\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 5s 599ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4160 - accuracy: 0.7795\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 5s 597ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3647 - accuracy: 0.7859\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 5s 593ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3190 - accuracy: 0.7906\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 5s 592ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2838 - accuracy: 0.7944\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 5s 591ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2586 - accuracy: 0.7974\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2396 - accuracy: 0.7997\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2237 - accuracy: 0.8016\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2118 - accuracy: 0.8035\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 5s 587ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2013 - accuracy: 0.8046\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1936 - accuracy: 0.8052\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1871 - accuracy: 0.8060\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1805 - accuracy: 0.8067\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1742 - accuracy: 0.8070\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1710 - accuracy: 0.8072\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1671 - accuracy: 0.8077\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 5s 591ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1646 - accuracy: 0.8080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_CNN_Accuracy: 80.240059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_CNN_Test_Accuracy y: 0.654594\n",
            "ELMo20_CNN_Precision: 0.654594\n",
            "ELMo20_CNN_Recall: 0.654594\n",
            "ELMo20_CNN_F1_Score: 0.654594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 6s 585ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6570 - accuracy: 0.8031\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 5s 587ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5920 - accuracy: 0.8034\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 5s 591ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5470 - accuracy: 0.8040\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 5s 593ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5030 - accuracy: 0.8060\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 5s 596ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4581 - accuracy: 0.8078\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 5s 596ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4152 - accuracy: 0.8089\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 5s 595ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3780 - accuracy: 0.8100\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 5s 593ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3487 - accuracy: 0.8117\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 5s 592ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3259 - accuracy: 0.8135\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 5s 590ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3059 - accuracy: 0.8153\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 5s 589ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2896 - accuracy: 0.8169\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2755 - accuracy: 0.8183\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2646 - accuracy: 0.8192\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2535 - accuracy: 0.8202\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2467 - accuracy: 0.8208\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 5s 586ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2397 - accuracy: 0.8213\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2343 - accuracy: 0.8218\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 5s 589ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2299 - accuracy: 0.8220\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 5s 588ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2268 - accuracy: 0.8223\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 5s 589ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2229 - accuracy: 0.8225\n",
            "ELMo30_CNN_Accuracy: 70.807064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_CNN_Test_Accuracy: 0.649293\n",
            "ELMo30_CNN_Precision: 0.649293\n",
            "ELMo30_CNN_Recall: 0.649293\n",
            "ELMo30_CNN_F1_Score: 0.649293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELMo + BiLSTM Models"
      ],
      "metadata": {
        "id": "EMf-Ar9LGRAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env TF_ALLOCATOR_MAX_ALLOCATION_SIZE=10G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6AK_v5MxgRs",
        "outputId": "1e5ed544-0ec7-442b-e988-151ea27a540e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_ALLOCATOR_MAX_ALLOCATION_SIZE=10G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optparse import Option\n",
        "# Tuning and Define the BiLSTM models\n",
        "batch_sizes = 16\n",
        "\n",
        "def ten_word_bilstm_model():\n",
        "  bilstm_model = Sequential()\n",
        "  bilstm_model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  bilstm_model.add(Bidirectional(LSTM(bilstm_memory, return_sequences = False)))\n",
        "  bilstm_model.add(Dropout(dropout))\n",
        "  bilstm_model.add(Dense(ten_num_classes, activation='softmax'))\n",
        "  bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  bilstm_model.summary()\n",
        "  return bilstm_model\n",
        "\n",
        "def twenty_word_bilstm_model():\n",
        "  bilstm_model = Sequential()\n",
        "  bilstm_model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  bilstm_model.add(Bidirectional(LSTM(bilstm_memory, return_sequences = False)))\n",
        "  bilstm_model.add(Dropout(dropout))\n",
        "  bilstm_model.add(Dense(twenty_num_classes, activation='softmax'))\n",
        "  bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  bilstm_model.summary()\n",
        "  return bilstm_model\n",
        "\n",
        "def thirty_word_bilstm_model():\n",
        "  bilstm_model = Sequential()\n",
        "  bilstm_model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  bilstm_model.add(Bidirectional(LSTM(bilstm_memory, return_sequences = False)))\n",
        "  bilstm_model.add(Dropout(dropout))\n",
        "  bilstm_model.add(Dense(thirty_num_classes, activation='softmax'))\n",
        "  # Compile the model\n",
        "  bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  bilstm_model.summary()\n",
        "  return bilstm_model\n",
        "\n",
        "\n",
        "bilstm_model1 = ten_word_bilstm_model()\n",
        "bilstm_model2 = twenty_word_bilstm_model()\n",
        "bilstm_model3 = thirty_word_bilstm_model()\n",
        "\n",
        "bilstm_model1.save(\"ten_word_bilstm_model\")\n",
        "bilstm_model1.save(\"twenty_word_bilstm_model\")\n",
        "bilstm_model1.save(\"thirty_word_bilstm_model\")\n",
        "\n",
        "\n",
        "##=================10words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history10 = bilstm_model1.fit(X_train, y10_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo10_BiLSTM_Accuracy = bilstm_model1.evaluate(X_train, y10_train, verbose=1)\n",
        "    print(('ELMo10_BiLSTM_Accuracy: %f' % (ELMo10_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "\n",
        "##===============================ELMo10_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = bilstm_model1.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo10_BiLSTM_Test_Accuracy = accuracy_score(y10_test, np.round(abs(predictions)))\n",
        "    print('ELMo10_BiLSTM_Test_Accuracy: %f' % ELMo10_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo10_BiLSTM_precision = precision_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_BiLSTM_precision: %f' % ELMo10_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo10_BiLSTM_Recall = recall_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_BiLSTM_Recall: %f' % ELMo10_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo10_BiLSTM_F1_Score = f1_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_BiLSTM_F1_Score: %f' % ELMo10_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "bilstm_data10 = {\n",
        "    'Name': [\"ELMo10_BiLSTM_Accuracy\", \"ELMo10_BiLSTM_Test_Accuracy\", \"ELMo10_BiLSTM_precision\", \"ELMo10_BiLSTM_Recall\", \"ELMo10_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo10_BiLSTM_Accuracy, ELMo10_BiLSTM_Test_Accuracy, ELMo10_BiLSTM_precision, ELMo10_BiLSTM_Recall, ELMo10_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(bilstm_data10)\n",
        "df.to_excel('ELMo10_BiLSTM_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "##=================20words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history20 = bilstm_model2.fit(X_train, y20_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo20_BiLSTM_Accuracy = bilstm_model2.evaluate(X_train, y20_train, verbose=1)\n",
        "    print(('ELMo20_BiLSTM_Accuracy: %f' % (ELMo20_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "\n",
        "##===============================ELMo20_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = bilstm_model2.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo20_BiLSTM_Test_Accuracy = accuracy_score(y20_test, np.round(abs(predictions)))\n",
        "    print('ELMo20_BiLSTM_Test_Accuracy: %f' % ELMo20_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo20_BiLSTM_precision = precision_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_BiLSTM_precision: %f' % ELMo20_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo20_BiLSTM_Recall = recall_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_BiLSTM_Recall: %f' % ELMo20_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo20_BiLSTM_F1_Score = f1_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_BiLSTM_F1_Score: %f' % ELMo20_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "bilstm_data20 = {\n",
        "    'Name': [\"ELMo20_BiLSTM_Accuracy\", \"ELMo20_BiLSTM_Test_Accuracy\", \"ELMo20_BiLSTM_precision\", \"ELMo20_BiLSTM_Recall\", \"ELMo20_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo20_BiLSTM_Accuracy, ELMo20_BiLSTM_Test_Accuracy, ELMo20_BiLSTM_precision, ELMo20_BiLSTM_Recall, ELMo20_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(bilstm_data20)\n",
        "df.to_excel('ELMo20_BiLSTM_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "##=================30words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history30 = bilstm_model3.fit(X_train, y30_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo30_BiLSTM_Accuracy = bilstm_model3.evaluate(X_train, y30_train, verbose=1)\n",
        "    print(('ELMo30_BiLSTM_Accuracy: %f' % (ELMo30_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "\n",
        "##===============================ELMo30_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = bilstm_model3.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo30_BiLSTM_Test_Accuracy = accuracy_score(y30_test, np.round(abs(predictions)))\n",
        "    print('ELMo30_BiLSTM_Test_Accuracy: %f' % ELMo30_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo30_BiLSTM_precision = precision_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_BiLSTM_precision: %f' % ELMo30_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo30_BiLSTM_Recall = recall_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_BiLSTM_Recall: %f' % ELMo30_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo30_BiLSTM_F1_Score = f1_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_BiLSTM_F1_Score: %f' % ELMo30_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "bilstm_data30 = {\n",
        "    'Name': [\"ELMo30_BiLSTM_Accuracy\", \"ELMo30_BiLSTM_Test_Accuracy\", \"ELMo30_BiLSTM_precision\", \"ELMo30_BiLSTM_Recall\", \"ELMo30_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo30_BiLSTM_Accuracy, ELMo30_BiLSTM_Test_Accuracy, ELMo30_BiLSTM_precision, ELMo30_BiLSTM_Recall, ELMo30_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(bilstm_data30)\n",
        "df.to_excel('ELMo30_BiLSTM_Scores_file.xlsx', index=False)"
      ],
      "metadata": {
        "id": "8YPJf3VpGQni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683dd99c-8bfc-433f-932c-2d1d01cac261"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 100)               122800    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3444130 (13.14 MB)\n",
            "Trainable params: 3444130 (13.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 100)               122800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 20)                2020      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3445140 (13.14 MB)\n",
            "Trainable params: 3445140 (13.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 100)               122800    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 30)                3030      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3446150 (13.15 MB)\n",
            "Trainable params: 3446150 (13.15 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 6s 487ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6729 - accuracy: 0.7081\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 4s 488ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6051 - accuracy: 0.7088\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 4s 488ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5732 - accuracy: 0.7132\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 4s 487ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5548 - accuracy: 0.7096\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 4s 489ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5240 - accuracy: 0.7180\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 4s 490ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4956 - accuracy: 0.7217\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 4s 489ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4706 - accuracy: 0.7249\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 4s 492ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4435 - accuracy: 0.7299\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 4s 492ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4179 - accuracy: 0.7335\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 4s 494ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3948 - accuracy: 0.7370\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3729 - accuracy: 0.7423\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3520 - accuracy: 0.7462\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3332 - accuracy: 0.7501\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3164 - accuracy: 0.7546\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 4s 493ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3001 - accuracy: 0.7578\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 4s 491ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2850 - accuracy: 0.7606\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 4s 492ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2706 - accuracy: 0.7639\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 4s 490ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2579 - accuracy: 0.7662\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 4s 490ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2465 - accuracy: 0.7683\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 4s 491ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2338 - accuracy: 0.7711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_BiLSTM_Accuracy: 77.260691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_BiLSTM_Test_Accuracy: 0.678445\n",
            "ELMo10_BiLSTM_precision: 0.678445\n",
            "ELMo10_BiLSTM_Recall: 0.678445\n",
            "ELMo10_BiLSTM_F1_Score: 0.678445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 4s 340ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6782 - accuracy: 0.7690\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 3s 345ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6050 - accuracy: 0.7690\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 3s 342ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5425 - accuracy: 0.7696\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 3s 345ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5279 - accuracy: 0.7694\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 3s 347ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5197 - accuracy: 0.7691\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 3s 347ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5104 - accuracy: 0.7691\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 3s 348ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4983 - accuracy: 0.7692\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4848 - accuracy: 0.7697\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4703 - accuracy: 0.7704\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4567 - accuracy: 0.7713\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4428 - accuracy: 0.7725\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 3s 351ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4287 - accuracy: 0.7736\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4147 - accuracy: 0.7748\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 3s 353ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4010 - accuracy: 0.7758\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3889 - accuracy: 0.7764\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 3s 351ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3770 - accuracy: 0.7776\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 3s 351ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3660 - accuracy: 0.7785\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 3s 348ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3547 - accuracy: 0.7796\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3445 - accuracy: 0.7802\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 3s 347ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3349 - accuracy: 0.7813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_BiLSTM_Accuracy: 78.053033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_BiLSTM_Test_Accuracy: 0.654594\n",
            "ELMo20_BiLSTM_precision: 0.654594\n",
            "ELMo20_BiLSTM_Recall: 0.654594\n",
            "ELMo20_BiLSTM_F1_Score: 0.654594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 4s 342ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6709 - accuracy: 0.8031\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 3s 343ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5782 - accuracy: 0.8031\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 3s 343ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5110 - accuracy: 0.8037\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 3s 344ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4923 - accuracy: 0.8032\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 3s 347ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4851 - accuracy: 0.8031\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 3s 346ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4801 - accuracy: 0.8031\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 3s 348ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4726 - accuracy: 0.8031\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 3s 349ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4628 - accuracy: 0.8031\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 3s 349ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4530 - accuracy: 0.8031\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 3s 351ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4429 - accuracy: 0.8032\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4334 - accuracy: 0.8033\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 3s 353ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4249 - accuracy: 0.8033\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 3s 353ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4162 - accuracy: 0.8035\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 3s 354ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4073 - accuracy: 0.8037\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 3s 355ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3992 - accuracy: 0.8038\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3910 - accuracy: 0.8040\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3832 - accuracy: 0.8041\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 3s 352ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3754 - accuracy: 0.8043\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 3s 350ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3681 - accuracy: 0.8045\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 3s 349ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3614 - accuracy: 0.8047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_BiLSTM_Accuracy: 80.443805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_BiLSTM_Test_Accuracy: 0.649293\n",
            "ELMo30_BiLSTM_precision: 0.649293\n",
            "ELMo30_BiLSTM_Recall: 0.649293\n",
            "ELMo30_BiLSTM_F1_Score: 0.649293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ELMo+CNN_BiLSTM Models"
      ],
      "metadata": {
        "id": "B4FzLFwPVj7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.disable_eager_execution()"
      ],
      "metadata": {
        "id": "tPCoqp-N714K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ten_word_cnn_bilstm_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  model.add(Conv1D(128, kernel_size = 5, input_shape = (max_sequence_length, vocab_size), activation = 'relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "  model.add(Dense(ten_num_classes, activation = 'softmax'))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def twenty_word_cnn_bilstm_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  model.add(Conv1D(filter_sizes, kernel_size = kernel_sizes, input_shape = (max_sequence_length, vocab_size), activation = 'relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Bidirectional(LSTM(lstm_memory, return_sequences=False)))\n",
        "  model.add(Dense(twenty_num_classes, activation = 'softmax'))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def thirty_word_cnn_bilstm_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=bilstm_embedding_dim, input_length=max_sequence_length))\n",
        "  model.add(Conv1D(filter_sizes, kernel_size = kernel_sizes, input_shape = (max_sequence_length, vocab_size), activation = 'relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Bidirectional(LSTM(lstm_memory, return_sequences=False)))\n",
        "  model.add(Dense(thirty_num_classes, activation = 'softmax'))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "cnn_bilstm_model1 = ten_word_cnn_bilstm_model()\n",
        "cnn_bilstm_model2 = twenty_word_cnn_bilstm_model()\n",
        "cnn_bilstm_model3 = thirty_word_cnn_bilstm_model()\n",
        "\n",
        "cnn_bilstm_model1.save('ten_word_cnn_bilstm_model')\n",
        "cnn_bilstm_model2.save('twenty_word_cnn_bilstm_model')\n",
        "cnn_bilstm_model3.save('thirty_word_cnn_bilstm_model')\n",
        "\n",
        "\n",
        "##=================10words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history10 = cnn_bilstm_model1.fit(X_train, y10_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo10_CNN_BiLSTM_Accuracy = cnn_bilstm_model1.evaluate(X_train, y10_train, verbose=1)\n",
        "    print(('ELMo10_CNN_BiLSTM_Accuracy: %f' % (ELMo10_CNN_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "##===============================ELMo10_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = cnn_bilstm_model1.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo10_CNN_BiLSTM_Test_Accuracy = accuracy_score(y10_test, np.round(abs(predictions)))\n",
        "    print('ELMo10_CNN_BiLSTM_Test_Accuracy: %f' % ELMo10_CNN_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo10_CNN_BiLSTM_precision = precision_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_BiLSTM_precision: %f' % ELMo10_CNN_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo10_CNN_BiLSTM_Recall = recall_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_BiLSTM_Recall: %f' % ELMo10_CNN_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo10_CNN_BiLSTM_F1_Score = f1_score(y10_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo10_CNN_BiLSTM_F1_Score: %f' % ELMo10_CNN_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "cnn_bilstm_data10 = {\n",
        "    'Name': [\"ELMo10_CNN_BiLSTM_Accuracy\", \"ELMo10_CNN_BiLSTM_Test_Accuracy\", \"ELMo10_CNN_BiLSTM_precision\", \"ELMo10_CNN_BiLSTM_Recall\", \"ELMo10_CNN_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo10_CNN_BiLSTM_Accuracy, ELMo10_CNN_BiLSTM_Test_Accuracy, ELMo10_CNN_BiLSTM_precision, ELMo10_CNN_BiLSTM_Recall, ELMo10_CNN_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(cnn_bilstm_data10)\n",
        "df.to_excel('ELMo10_CNN_BiLSTM_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "\n",
        "##=================20words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history20 = cnn_bilstm_model2.fit(X_train, y20_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo20_CNN_BiLSTM_Accuracy = cnn_bilstm_model2.evaluate(X_train, y20_train, verbose=1)\n",
        "    print(('ELMo20_CNN_BiLSTM_Accuracy: %f' % (ELMo20_CNN_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "##===============================ELMo20_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = cnn_bilstm_model2.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo20_CNN_BiLSTM_Test_Accuracy = accuracy_score(y20_test, np.round(abs(predictions)))\n",
        "    print('ELMo20_CNN_BiLSTM_Test_Accuracy: %f' % ELMo20_CNN_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo20_CNN_BiLSTM_precision = precision_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_BiLSTM__precision: %f' % ELMo20_CNN_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo20_CNN_BiLSTM_Recall = recall_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_BiLSTM_Recall: %f' % ELMo20_CNN_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo20_CNN_BiLSTM_F1_Score = f1_score(y20_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo20_CNN_BiLSTM_F1_Score: %f' % ELMo20_CNN_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "cnn_bilstm_data20 = {\n",
        "    'Name': [\"ELMo20_CNN_BiLSTM_Accuracy\", \"ELMo20_CNN_BiLSTM_Test_Accuracy\", \"ELMo20_CNN_BiLSTM_precision\", \"ELMo20_CNN_BiLSTM_Recall\", \"ELMo20_CNN_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo20_CNN_BiLSTM_Accuracy, ELMo20_CNN_BiLSTM_Test_Accuracy, ELMo20_CNN_BiLSTM_precision, ELMo20_CNN_BiLSTM_Recall, ELMo20_CNN_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(cnn_bilstm_data20)\n",
        "df.to_excel('ELMo20_CNN_BiLSTM_Scores_file.xlsx', index=False)\n",
        "\n",
        "\n",
        "\n",
        "##=================30words model training=====================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    history30 = cnn_bilstm_model3.fit(X_train, y30_train, epochs=epochs, batch_size=batch_sizes, steps_per_epoch=8)\n",
        "\n",
        "    ELMo30_CNN_BiLSTM_Accuracy = cnn_bilstm_model3.evaluate(X_train, y30_train, verbose=1)\n",
        "    print(('ELMo30_CNN_BiLSTM_Accuracy: %f' % (ELMo30_CNN_BiLSTM_Accuracy[1] * 100)))\n",
        "\n",
        "##===============================ELMo30_CNN_Model Evaluation========================================\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    # Here X_test, y_test are the test data points\n",
        "    predictions = bilstm_model3.predict(X_test)\n",
        "    predictions = predictions[:, 0]\n",
        "\n",
        "    # CBOW Tst Accuracy\n",
        "    ELMo30_CNN_BiLSTM_Test_Accuracy = accuracy_score(y30_test, np.round(abs(predictions)))\n",
        "    print('ELMo30_CNN_BiLSTM_Test_Accuracy: %f' % ELMo30_CNN_BiLSTM_Test_Accuracy)\n",
        "\n",
        "    # Calculating the precision score of classifier\n",
        "    ELMo30_CNN_BiLSTM_precision = precision_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_BiLSTM_precision: %f' % ELMo30_CNN_BiLSTM_precision)\n",
        "\n",
        "    # Calculating the recall score of classifier\n",
        "    ELMo30_CNN_BiLSTM_Recall = recall_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_BiLSTM_Recall: %f' % ELMo30_CNN_BiLSTM_Recall)\n",
        "\n",
        "    # Calculating the F1 score of classifier\n",
        "    ELMo30_CNN_BiLSTM_F1_Score = f1_score(y30_test, np.round(abs(predictions)), average='micro')\n",
        "    print('ELMo30_CNN_BiLSTM_F1_Score: %f' % ELMo30_CNN_BiLSTM_F1_Score)\n",
        "\n",
        "\n",
        "cnn_bilstm_data30 = {\n",
        "    'Name': [\"ELMo30_CNN_BiLSTM_Accuracy\", \"ELMo30_CNN_BiLSTM_Test_Accuracy\", \"ELMo30_CNN_BiLSTM_precision\", \"ELMo30_CNN_BiLSTM_Recall\", \"ELMo30_CNN_BiLSTM_F1_Score\"],\n",
        "    'Scores': [ELMo30_CNN_BiLSTM_Accuracy, ELMo30_CNN_BiLSTM_Test_Accuracy, ELMo30_CNN_BiLSTM_precision, ELMo30_CNN_BiLSTM_Recall, ELMo30_CNN_BiLSTM_F1_Score]\n",
        "}\n",
        "df = pd.DataFrame(cnn_bilstm_data30)\n",
        "df.to_excel('ELMo30_CNN_BiLSTM_Scores_file.xlsx', index=False)"
      ],
      "metadata": {
        "id": "TLcZIOMmgwW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e79d0e7-ddf8-43d7-88ce-1ae918b0bf2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 152, 128)          163968    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 76, 128)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 76, 128)           0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3584394 (13.67 MB)\n",
            "Trainable params: 3584394 (13.67 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 152, 128)          163968    \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 76, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 76, 128)           0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 20)                2580      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3585684 (13.68 MB)\n",
            "Trainable params: 3585684 (13.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_11 (Embedding)    (None, 156, 256)          3320320   \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 152, 128)          163968    \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, 76, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 76, 128)           0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 30)                3870      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3586974 (13.68 MB)\n",
            "Trainable params: 3586974 (13.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 9s 493ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6360 - accuracy: 0.7081\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 4s 494ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5645 - accuracy: 0.7083\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 4s 494ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5497 - accuracy: 0.7081\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5243 - accuracy: 0.7081\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4766 - accuracy: 0.7087\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4252 - accuracy: 0.7181\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3729 - accuracy: 0.7341\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3210 - accuracy: 0.7503\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2719 - accuracy: 0.7624\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2273 - accuracy: 0.7736\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1906 - accuracy: 0.7790\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1605 - accuracy: 0.7825\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1367 - accuracy: 0.7843\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1180 - accuracy: 0.7854\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1029 - accuracy: 0.7858\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.0906 - accuracy: 0.7860\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.0805 - accuracy: 0.7861\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.0718 - accuracy: 0.7852\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.0643 - accuracy: 0.7857\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.0581 - accuracy: 0.7855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_CNN_BiLSTM_Accuracy: 78.106046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_CNN_BiLSTM_Test_Accuracy: 0.678445\n",
            "ELMo10_CNN_BiLSTM_precision: 0.678445\n",
            "ELMo10_CNN_BiLSTM_Recall: 0.678445\n",
            "ELMo10_CNN_BiLSTM_F1_Score: 0.678445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 6s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6328 - accuracy: 0.7690\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 4s 494ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5245 - accuracy: 0.7690\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5061 - accuracy: 0.7690\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4973 - accuracy: 0.7690\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4866 - accuracy: 0.7690\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4724 - accuracy: 0.7690\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4487 - accuracy: 0.7691\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4142 - accuracy: 0.7692\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 4s 502ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3784 - accuracy: 0.7706\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3456 - accuracy: 0.7752\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3154 - accuracy: 0.7804\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 4s 501ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2890 - accuracy: 0.7848\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2651 - accuracy: 0.7886\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2433 - accuracy: 0.7914\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2239 - accuracy: 0.7937\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2061 - accuracy: 0.7954\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1899 - accuracy: 0.7968\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1755 - accuracy: 0.7977\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1623 - accuracy: 0.7985\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.1507 - accuracy: 0.7993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_CNN_BiLSTM_Accuracy: 79.839474\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo20_CNN_BiLSTM_Test_Accuracy: 0.654594\n",
            "ELMo20_CNN_BiLSTM__precision: 0.654594\n",
            "ELMo20_CNN_BiLSTM_Recall: 0.654594\n",
            "ELMo20_CNN_BiLSTM_F1_Score: 0.654594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8 samples\n",
            "Epoch 1/20\n",
            "8/8 [==============================] - 6s 493ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.6435 - accuracy: 0.8031\n",
            "Epoch 2/20\n",
            "8/8 [==============================] - 4s 493ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.5113 - accuracy: 0.8031\n",
            "Epoch 3/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4707 - accuracy: 0.8031\n",
            "Epoch 4/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4673 - accuracy: 0.8031\n",
            "Epoch 5/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4622 - accuracy: 0.8031\n",
            "Epoch 6/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4554 - accuracy: 0.8031\n",
            "Epoch 7/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4455 - accuracy: 0.8031\n",
            "Epoch 8/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4336 - accuracy: 0.8031\n",
            "Epoch 9/20\n",
            "8/8 [==============================] - 4s 501ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4209 - accuracy: 0.8031\n",
            "Epoch 10/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.4063 - accuracy: 0.8031\n",
            "Epoch 11/20\n",
            "8/8 [==============================] - 4s 501ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3902 - accuracy: 0.8036\n",
            "Epoch 12/20\n",
            "8/8 [==============================] - 4s 499ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3734 - accuracy: 0.8052\n",
            "Epoch 13/20\n",
            "8/8 [==============================] - 4s 500ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3563 - accuracy: 0.8070\n",
            "Epoch 14/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3394 - accuracy: 0.8086\n",
            "Epoch 15/20\n",
            "8/8 [==============================] - 4s 497ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3227 - accuracy: 0.8102\n",
            "Epoch 16/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.3068 - accuracy: 0.8113\n",
            "Epoch 17/20\n",
            "8/8 [==============================] - 4s 498ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2922 - accuracy: 0.8121\n",
            "Epoch 18/20\n",
            "8/8 [==============================] - 4s 494ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2786 - accuracy: 0.8133\n",
            "Epoch 19/20\n",
            "8/8 [==============================] - 4s 496ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2657 - accuracy: 0.8142\n",
            "Epoch 20/20\n",
            "8/8 [==============================] - 4s 495ms/step - batch: 3.5000 - size: 1.0000 - loss: 0.2538 - accuracy: 0.8150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_CNN_BiLSTM_Accuracy: 81.561118\n",
            "ELMo30_CNN_BiLSTM_Test_Accuracy: 0.649293\n",
            "ELMo30_CNN_BiLSTM_precision: 0.649293\n",
            "ELMo30_CNN_BiLSTM_Recall: 0.649293\n",
            "ELMo30_CNN_BiLSTM_F1_Score: 0.649293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#======================================================train_labels==========================================================================\n",
        "ten_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud']\n",
        "twenty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource']\n",
        "thirty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource',\n",
        "               'environment', 'algorithm', 'energy', 'management', 'architecture', 'access', 'scheme', 'communication', 'execution', 'order']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##==============================10-words label==================================\n",
        "ten_words_labels = []\n",
        "\n",
        "for i in range(0, 1132):\n",
        "    count = 0\n",
        "    for j in range(0, len(ten_words)):\n",
        "        if ten_words[j] in test_data[i]:\n",
        "            count += 1\n",
        "    if count >=1:\n",
        "        ten_words_labels.append(1)\n",
        "    else:\n",
        "        ten_words_labels.append(0)\n",
        "\n",
        "print(\"ten_words_labels:\", len(ten_words_labels))\n",
        "\n",
        "##==============================20-words label==================================\n",
        "twenty_words_labels = []\n",
        "\n",
        "for i in range(0, 1132):\n",
        "    count = 0\n",
        "    for j in range(0, len(twenty_words)):\n",
        "        if twenty_words[j] in test_data[i]:\n",
        "            count += 1\n",
        "    if count >=1:\n",
        "        twenty_words_labels.append(1)\n",
        "    else:\n",
        "        twenty_words_labels.append(0)\n",
        "\n",
        "print(\"twenty_words_labels:\", len(twenty_words_labels))\n",
        "\n",
        "##==============================30-words label==================================\n",
        "thrity_words_labels = []\n",
        "\n",
        "for i in range(0, 1132):\n",
        "    count = 0\n",
        "    for j in range(0, len(thirty_words)):\n",
        "        if thirty_words[j] in test_data[i]:\n",
        "            count += 1\n",
        "    if count >=1:\n",
        "        thrity_words_labels.append(1)\n",
        "    else:\n",
        "        thrity_words_labels.append(0)\n",
        "\n",
        "print(\"thrity_words_labels:\", len(thrity_words_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbx2_auIBVCh",
        "outputId": "6a79d7dd-d72f-41f7-b200-23dbdc6f218d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ten_words_labels: 1132\n",
            "twenty_words_labels: 1132\n",
            "thrity_words_labels: 1132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.experimental.output_all_intermediates(True) #Graph execution for BiLSTM Models"
      ],
      "metadata": {
        "id": "uvCksfFoLVm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##============================BILSTM_Model_ROC_AuC_Scores======================================================\n",
        "positive = 1\n",
        "negative = 0\n",
        "none = None\n",
        "micro = 'micro'\n",
        "macro = 'macro'\n",
        "weight = \"weighted\"\n",
        "\n",
        "\n",
        "predictions =cnn_model1.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo10_cnn_roc_auc_Score = roc_auc_score(ten_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo10_cnn_roc_auc_Score: %f' % ELMo10_cnn_roc_auc_Score)\n",
        "\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =cnn_model2.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo20_cnn_roc_auc_Score = roc_auc_score(twenty_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo20_cnn_roc_auc_Score: %f' % ELMo20_cnn_roc_auc_Score)\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =cnn_model3.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo30_cnn_roc_auc_Score = roc_auc_score(thrity_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo30_cnn_roc_auc_Score: %f' % ELMo30_cnn_roc_auc_Score)\n",
        "\n",
        "\n",
        "##============================BILSTM_Model_ROC_AuC_Scores======================================================\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =bilstm_model1.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo10_bilstm_roc_auc_Score = roc_auc_score(ten_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo10_bilstm_roc_auc_Score: %f' % ELMo10_bilstm_roc_auc_Score)\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =bilstm_model2.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo20_bilstm_roc_auc_Score = roc_auc_score(twenty_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo20_bilstm_roc_auc_Score: %f' % ELMo20_bilstm_roc_auc_Score)\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =bilstm_model3.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo30_bilstm_roc_auc_Score = roc_auc_score(thrity_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo30_bilstm_roc_auc_Score: %f' % ELMo30_bilstm_roc_auc_Score)\n",
        "\n",
        "##============================CNN_BILSTM_Model_ROC_AuC_Scores======================================================\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =cnn_bilstm_model1.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo10_cnn_bilstm_roc_auc_Score = roc_auc_score(ten_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo10_cnn_bilstm_roc_auc_Score: %f' % ELMo10_cnn_bilstm_roc_auc_Score)\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =cnn_bilstm_model2.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo20_cnn_bilstm_roc_auc_Score = roc_auc_score(twenty_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo20_cnn_bilstm_roc_auc_Score: %f' % ELMo20_cnn_bilstm_roc_auc_Score)\n",
        "\n",
        "# Here X_test, y_test are the test data points\n",
        "predictions =cnn_bilstm_model3.predict(X_test)\n",
        "predictions = predictions[:, positive]\n",
        "\n",
        "#Calculating the F1 score of classifier\n",
        "ELMo30_cnn_bilstm_roc_auc_Score = roc_auc_score(thrity_words_labels, np.round(abs(predictions)), average=weight)\n",
        "print('ELMo30_cnn_bilstm_roc_auc_Score: %f' % ELMo30_cnn_bilstm_roc_auc_Score)\n",
        "\n",
        "roc_data = {\n",
        "    'Name': ['ELMo10_cnn_roc_auc_Score', 'ELMo20_cnn_roc_auc_Score', 'ELMo30_cnn_roc_auc_Score', 'ELMo10_bilstm_roc_auc_Score', 'ELMo20_bilstm_roc_auc_Score', 'ELMo30_bilstm_roc_auc_Score', 'ELMo10_cnn_bilstm_roc_auc_Score', 'ELMo20_cnn_bilstm_roc_auc_Score', 'ELMo30_cnn_bilstm_roc_auc_Score'],\n",
        "    'Scores': [ELMo10_cnn_roc_auc_Score, ELMo20_cnn_roc_auc_Score, ELMo30_cnn_roc_auc_Score, ELMo10_bilstm_roc_auc_Score, ELMo20_bilstm_roc_auc_Score, ELMo30_bilstm_roc_auc_Score, ELMo10_cnn_bilstm_roc_auc_Score, ELMo20_cnn_bilstm_roc_auc_Score, ELMo30_cnn_bilstm_roc_auc_Score]\n",
        "}\n",
        "df = pd.DataFrame(roc_data)\n",
        "df.to_excel('ELMo_ROC_AUC_Scores_file.xlsx', index=False)"
      ],
      "metadata": {
        "id": "CFlmSrHGWH-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989ff327-354b-4129-b064-62e62b40add0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_cnn_roc_auc_Score: 0.500000\n",
            "ELMo20_cnn_roc_auc_Score: 0.500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_cnn_roc_auc_Score: 0.500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_bilstm_roc_auc_Score: 0.500000\n",
            "ELMo20_bilstm_roc_auc_Score: 0.500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo30_bilstm_roc_auc_Score: 0.500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ELMo10_cnn_bilstm_roc_auc_Score: 0.500000\n",
            "ELMo20_cnn_bilstm_roc_auc_Score: 0.500000\n",
            "ELMo30_cnn_bilstm_roc_auc_Score: 0.500000\n"
          ]
        }
      ]
    }
  ]
}