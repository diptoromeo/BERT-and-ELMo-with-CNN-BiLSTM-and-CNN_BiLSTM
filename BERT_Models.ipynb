{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diptoromeo/BERT-and-ELMo-with-CNN-BiLSTM-and-CNN_BiLSTM/blob/main/BERT_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "purwiotnWogl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094a46c4-26fd-43eb-935d-ac95ab3899d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.5.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.12.0->pytorch_lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "from random import seed\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import regex\n",
        "import numpy as np\n",
        "from keras.utils import pad_sequences\n",
        "from numpy.f2py.crackfortran import quiet\n",
        "from opt_einsum.backends import torch\n",
        "from pandas.core.common import flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from string import punctuation\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "# ================================================================================================\n",
        "from transformers import BertModel,BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install pytorch_lightning\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
        "from sklearn.metrics import roc_auc_score,f1_score, roc_curve, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPoas-ermOGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f24b12-b9c2-4cce-ca55-eed9a36e3ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# cup cuda checking\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmUeYuhjYNsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6272aa68-4a75-4efc-ef46-27b2360ebbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGCS data: 5659\n"
          ]
        }
      ],
      "source": [
        "# ===============================Nltk abstract_words tokenize======================================\n",
        "with open('/content/FGFSJournal.txt', 'rt', encoding='UTF8') as file:\n",
        "    FGFS_abstract = []\n",
        "    for line in file:\n",
        "        if '<abstract>' in line:\n",
        "            abstract = line.split('</abstract>')[0].split('<abstract>')[-1]\n",
        "            abstract = ''.join(i for i in abstract if not i.isdigit())\n",
        "            abstract = regex.sub('[^\\w\\d\\s]+', '', abstract)\n",
        "            ##abstract = nltk.sent_tokenize(abstract)\n",
        "            abstract = nltk.word_tokenize(abstract)\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            filtered_sentence_abstract = [w.lower() for w in abstract if\n",
        "                                          w.lower() not in punctuation and w.lower() not in stop_words]\n",
        "            tagged_list = nltk.pos_tag(filtered_sentence_abstract)\n",
        "            nouns_list = [t[0] for t in tagged_list if t[-1] == 'NN']\n",
        "            lm = WordNetLemmatizer()\n",
        "            singluar_form = [lm.lemmatize(w, pos='v') for w in nouns_list]\n",
        "            FGFS_abstract.append(singluar_form)\n",
        "\n",
        "print(\"FGCS data:\", len(FGFS_abstract))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrUmG004RhSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49969386-2239-464c-b37c-95c0ade6dc52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ten_words_labels: 5659\n",
            "twenty_words_labels: 5659\n",
            "thrity_words_labels: 5659\n"
          ]
        }
      ],
      "source": [
        "#======================================================train_labels==========================================================================\n",
        "ten_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud']\n",
        "twenty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource']\n",
        "thirty_words = ['paper', 'system', 'performance', 'network', 'model', 'service', 'time', 'information', 'approach', 'cloud',\n",
        "                'problem', 'process', 'security', 'analysis', 'application', 'method', 'research', 'framework', 'number', 'resource',\n",
        "               'environment', 'algorithm', 'energy', 'management', 'architecture', 'access', 'scheme', 'communication', 'execution', 'order']\n",
        "\n",
        "\n",
        "counts = 3\n",
        "\n",
        "##==============================10-words label==================================\n",
        "ten_words_labels = []\n",
        "\n",
        "for i in range(0, 5659):\n",
        "    count = 0\n",
        "    for j in range(0, len(ten_words)):\n",
        "        if ten_words[j] in FGFS_abstract[i]:\n",
        "            count += 1\n",
        "    if count >=counts:\n",
        "        ten_words_labels.append(1)\n",
        "    else:\n",
        "        ten_words_labels.append(0)\n",
        "\n",
        "print(\"ten_words_labels:\", len(ten_words_labels))\n",
        "\n",
        "##==============================20-words label==================================\n",
        "twenty_words_labels = []\n",
        "\n",
        "for i in range(0, 5659):\n",
        "    count = 0\n",
        "    for j in range(0, len(twenty_words)):\n",
        "        if twenty_words[j] in FGFS_abstract[i]:\n",
        "            count += 1\n",
        "    if count >=counts:\n",
        "        twenty_words_labels.append(1)\n",
        "    else:\n",
        "        twenty_words_labels.append(0)\n",
        "\n",
        "print(\"twenty_words_labels:\", len(twenty_words_labels))\n",
        "\n",
        "##==============================30-words label==================================\n",
        "thrity_words_labels = []\n",
        "\n",
        "for i in range(0, 5659):\n",
        "    count = 0\n",
        "    for j in range(0, len(thirty_words)):\n",
        "        if thirty_words[j] in FGFS_abstract[i]:\n",
        "            count += 1\n",
        "    if count >=counts:\n",
        "        thrity_words_labels.append(1)\n",
        "    else:\n",
        "        thrity_words_labels.append(0)\n",
        "\n",
        "print(\"thrity_words_labels:\", len(thrity_words_labels))\n",
        "\n",
        "\n",
        "# ##==============================10-words Multi-label==================================\n",
        "# ten_words_labels = []\n",
        "\n",
        "# for doc in FGFS_abstract:\n",
        "#     label = []\n",
        "#     for term in ten_words:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     ten_words_labels.append(label)\n",
        "\n",
        "# print(\"ten_words_labels:\", len(ten_words_labels))\n",
        "\n",
        "# ##==============================20-words Multi-label==================================\n",
        "# twenty_words_labels = []\n",
        "\n",
        "# for doc in FGFS_abstract:\n",
        "#     label = []\n",
        "#     for term in twenty_words:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     twenty_words_labels.append(label)\n",
        "\n",
        "# print(\"twenty_words_labels:\", len(twenty_words_labels))\n",
        "\n",
        "# ##==============================30-words Multi-label==================================\n",
        "# thrity_words_labels = []\n",
        "# for doc in FGFS_abstract:\n",
        "#     label = []\n",
        "#     for term in ten_words:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     thrity_words_labels.append(label)\n",
        "\n",
        "# print(\"thrity_words_labelss:\", len(thrity_words_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCy77ltIYPa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b26c09-105f-4150-8d43-b098033e6f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  growth\n",
            "Token IDs: tensor([  101,  4294, 28937,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "4,527 training samples\n",
            "1,132 validation samples\n"
          ]
        }
      ],
      "source": [
        "##=================================Bert_fine-tuning for sequence model==========================\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# cup cuda checking\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# ===============================================================================================\n",
        "# checking the sentences line that is 156\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in FGFS_abstract:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "##===============================================================================================\n",
        "\n",
        "# Doing attention masking\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every abstracts...\n",
        "for article in FGFS_abstract:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        article,  # Sentence to encode.\n",
        "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "        max_length=max_len,  # Pad & truncate all sentences.\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,  # Construct attn. masks.\n",
        "        return_tensors='pt',  # Return pytorch tensors.\n",
        "    )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "ten_labels = torch.tensor(ten_words_labels)\n",
        "twenty_labels = torch.tensor(twenty_words_labels)\n",
        "thrity_labels = torch.tensor(thrity_words_labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', article[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "\n",
        "##=========================================================================\n",
        "ten_words_labels = torch.tensor(ten_words_labels)\n",
        "twenty_words_labels = torch.tensor(twenty_words_labels)\n",
        "thrity_words_labels = torch.tensor(thrity_words_labels)\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset1 = TensorDataset(input_ids, attention_masks, ten_words_labels)\n",
        "dataset2 = TensorDataset(input_ids, attention_masks, twenty_words_labels)\n",
        "dataset3 = TensorDataset(input_ids, attention_masks, thrity_words_labels)\n",
        "\n",
        "# Create a 70-30 train-validation split.\n",
        "# Calculate the number of 10_words_samples to include in each set.\n",
        "train_size = int(0.7 * len(dataset1))\n",
        "val_size = int(0.3 * len(dataset1))\n",
        "val_size = len(dataset1) - train_size\n",
        "\n",
        "# Calculate the number of 20_words_samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset2))\n",
        "val_size = int(0.2 * len(dataset2))\n",
        "val_size = len(dataset2) - train_size\n",
        "\n",
        "# Calculate the number of 30_words_samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset3))\n",
        "val_size = int(0.2 * len(dataset3))\n",
        "val_size = len(dataset3) - train_size\n",
        "\n",
        "\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset1, val_dataset1 = random_split(dataset1, [train_size, val_size])\n",
        "train_dataset2, val_dataset2 = random_split(dataset2, [train_size, val_size])\n",
        "train_dataset3, val_dataset3 = random_split(dataset3, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "\n",
        "##===============================================================================================\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training 10_words samples in random order.\n",
        "train_dataloader1 = DataLoader(\n",
        "            train_dataset1,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset1), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader1 = DataLoader(\n",
        "            val_dataset1, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset1), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "##================================================================================================\n",
        "# We'll take training 20_words samples in random order.\n",
        "train_dataloader2 = DataLoader(\n",
        "            train_dataset2,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset2), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader2 = DataLoader(\n",
        "            val_dataset2, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset2), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )\n",
        "\n",
        "##================================================================================================\n",
        "# We'll take training 30_words samples in random order.\n",
        "train_dataloader3 = DataLoader(\n",
        "            train_dataset3,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset3), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader3 = DataLoader(\n",
        "            val_dataset3, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset3), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyKuF5WbYuPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff609d3-c75e-4ceb-b72b-7bf442a9bd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================================\n",
        "tuned_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "class BertCNNClassifier(nn.Module):\n",
        "    def __init__(self, tuned_model, embed_num = 512, embed_dim = 768, dropout=0.1, kernel_num=3, kernel_sizes=[1,2], num_labels=2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.embed_num = embed_num\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "        self.bert = tuned_model.bert\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, self.embed_dim)) for k in self.kernel_sizes])\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids = None):\n",
        "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids) #16,512,768\n",
        "        output = output[0].unsqueeze(1) #16,1,512,768\n",
        "        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs] #16,3,508,1 => #16,3,508\n",
        "        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output] #=> 16,3\n",
        "        output = torch.cat(output, 1)\n",
        "        output = self.dropout(output)\n",
        "        logits = self.classifier(output)\n",
        "        return self.softmax(logits, 1)\n",
        "\n",
        "# ======================================================================================\n",
        "\n",
        "# Initializing model\n",
        "model1 = BertCNNClassifier(tuned_model=tuned_model)\n",
        "model1.to(device)\n",
        "\n",
        "# ================================================================================\n",
        "# set parameters\n",
        "epochs = 4\n",
        "\n",
        "optimizer = torch.optim.AdamW(model1.parameters(),\n",
        "                  lr=5e-5,  # args.learning_rate - default is 5e-5.\n",
        "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# ===========================Fine-tuning model===========================================\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 4\n",
        "\n",
        "# 10_words number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps1 = len(train_dataloader1) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler1 = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                            num_training_steps=total_steps1)\n",
        "\n",
        "##================================================================================================\n",
        "# 20_words number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps2 = len(train_dataloader2) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler2 = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                            num_training_steps=total_steps2)\n",
        "\n",
        "##================================================================================================\n",
        "# 30_words number of training steps is [number of batches] x [number of epochs].\n",
        "total_steps3 = len(train_dataloader3) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler3 = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=0,  # Default value in run_glue.py\n",
        "                                            num_training_steps=total_steps3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62Sn213r3Bjx"
      },
      "source": [
        "BERT + CNN with Ten_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajg5wLbnY9m6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36b5c90-0edc-4914-8ab8-b6afd53ae295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNN_10_Words Accuracy: 12.604240282685513\n",
            "  BERT+CNN_10_Words Training loss: 0.5269608964136548\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_10_Words Accuracy: 14.43661971830986\n",
            "  BERT+CNN_10_Words Validation loss: 0.41661379580766383\n",
            "  BERT+CNN_10_Words This epoch took: 0:02:16\n",
            "\n",
            "  BERT+CNN_10_Words roc_auc score:  0.8899019497351516\n",
            "  BERT+CNN_10_Words F1 score: 0.9240596167494678\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNN_10_Words Accuracy: 14.614840989399294\n",
            "  BERT+CNN_10_Words Training loss: 0.41051221704735774\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_10_Words Accuracy: 14.338028169014084\n",
            "  BERT+CNN_10_Words Validation loss: 0.4167313349079078\n",
            "  BERT+CNN_10_Words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNN_10_Words roc_auc score:  0.8817697348295794\n",
            "  BERT+CNN_10_Words F1 score: 0.9197183098591549\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNN_10_Words Accuracy: 15.19434628975265\n",
            "  BERT+CNN_10_Words Training loss: 0.3721788280212416\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_10_Words Accuracy: 14.633802816901408\n",
            "  BERT+CNN_10_Words Validation loss: 0.3961730553230769\n",
            "  BERT+CNN_10_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_10_Words roc_auc score:  0.9029785384231457\n",
            "  BERT+CNN_10_Words F1 score: 0.9339019189765457\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNN_10_Words Accuracy: 14.93286219081272\n",
            "  BERT+CNN_10_Words Training loss: 0.3856823497020735\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_10_Words Accuracy: 15.23943661971831\n",
            "  BERT+CNN_10_Words Validation loss: 0.3583224734789889\n",
            "  BERT+CNN_10_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_10_Words roc_auc score:  0.9531612757804575\n",
            "  BERT+CNN_10_Words F1 score: 0.9626307922272048\n",
            "===\n",
            "Summary\n",
            "BERT+CNN_10_Words Total time 0:09:21 (h:mm:ss)\n",
            "BERT+CNN_10_Words best acc: 0.9558303886925795\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "## Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "##=====================================================================================\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model1.train()\n",
        "    for step, batch in enumerate(train_dataloader1):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model1.zero_grad()\n",
        "        out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader1)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader1)\n",
        "    print(\"  BERT+CNN_10_Words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+CNN_10_Words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model1.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader1)\n",
        "    print(\"  BERT+CNN_10_Words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader1)\n",
        "    print(\"  BERT+CNN_10_Words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNN_10_Words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNN_10_Words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNN_10_Words F1 score:', f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model1\n",
        "\n",
        "cnn10_fpr, cnn10_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNN_10_Words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNN_10_Words best acc:', accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-vnDVEz3JC0"
      },
      "source": [
        "BERT+CNN with Twenty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE8iUR_vY-bO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f4fd09-72f1-488f-b476-dea58d7e6c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNN_20_Words Accuracy: 14.063604240282686\n",
            "  Training loss: 0.43172905796829464\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_20_Words Accuracy: 11.295774647887324\n",
            "  BERT+CNN_20_Words Validation loss: 0.6052833023205609\n",
            "  BERT+CNN_20_Words  This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_20_Words  roc_auc score:  0.6466809421841542\n",
            "  BERT+CNN_20_Words  F1 score: 0.8012048192771084\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNN_20_Words Accuracy: 14.484098939929329\n",
            "  Training loss: 0.4086876134569148\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_20_Words Accuracy: 11.352112676056338\n",
            "  BERT+CNN_20_Words Validation loss: 0.601489782753125\n",
            "  BERT+CNN_20_Words  This epoch took: 0:02:23\n",
            "\n",
            "  BERT+CNN_20_Words  roc_auc score:  0.6509635974304069\n",
            "  BERT+CNN_20_Words  F1 score: 0.8031400966183575\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNN_20_Words Accuracy: 14.551236749116608\n",
            "  Training loss: 0.40555268641916686\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_20_Words Accuracy: 10.887323943661972\n",
            "  BERT+CNN_20_Words Validation loss: 0.6303212936495391\n",
            "  BERT+CNN_20_Words  This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_20_Words  roc_auc score:  0.6156316916488223\n",
            "  BERT+CNN_20_Words  F1 score: 0.7874481941977501\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNN_20_Words Accuracy: 14.636042402826854\n",
            "  Training loss: 0.3995166573633996\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_20_Words Accuracy: 11.098591549295774\n",
            "  BERT+CNN_20_Words Validation loss: 0.6170849006780437\n",
            "  BERT+CNN_20_Words  This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_20_Words  roc_auc score:  0.6316916488222698\n",
            "  BERT+CNN_20_Words  F1 score: 0.7945041816009557\n",
            "===\n",
            "Summary\n",
            "BERT+CNN_20_Words Total time 0:09:30 (h:mm:ss)\n",
            "BERT+CNN_20_Words best acc: 0.696113074204947\n"
          ]
        }
      ],
      "source": [
        "##=====================================================================================\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model1.train()\n",
        "    for step, batch in enumerate(train_dataloader2):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model1.zero_grad()\n",
        "        out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader2)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader2)\n",
        "    print(\"  BERT+CNN_20_Words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model1.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader2)\n",
        "    print(\"  BERT+CNN_20_Words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader2)\n",
        "    print(\"  BERT+CNN_20_Words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNN_20_Words  This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNN_20_Words  roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNN_20_Words  F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model1\n",
        "\n",
        "\n",
        "cnn20_fpr, cnn20_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNN_20_Words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNN_20_Words best acc:', accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoD6IDQR3Qr6"
      },
      "source": [
        "BERT+CNN with Thirty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGAkjlexw5_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211eea1a-c11d-4100-ce27-7b225badc8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNN_30_Words Accuracy: 15.070671378091873\n",
            "  BERT+CNN_30_Words Training loss: 0.3703057831252\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_30_Words Accuracy: 10.859154929577464\n",
            "  BERT+CNN_30_Words Validation loss: 0.633813974303259\n",
            "  BERT+CNN_30_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_30_Words roc_auc score:  0.6134903640256959\n",
            "  BERT+CNN_30_Words F1 score: 0.7865168539325843\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNN_30_Words Accuracy: 15.190812720848056\n",
            "  BERT+CNN_30_Words Training loss: 0.36428978765389947\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_30_Words Accuracy: 10.690140845070422\n",
            "  BERT+CNN_30_Words Validation loss: 0.6439179723531427\n",
            "  BERT+CNN_30_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_30_Words roc_auc score:  0.6006423982869379\n",
            "  BERT+CNN_30_Words F1 score: 0.7809747504403992\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNN_30_Words Accuracy: 15.074204946996467\n",
            "  BERT+CNN_30_Words Training loss: 0.3714211319866113\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_30_Words Accuracy: 10.295774647887324\n",
            "  BERT+CNN_30_Words Validation loss: 0.6790899630163757\n",
            "  BERT+CNN_30_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_30_Words roc_auc score:  0.5706638115631691\n",
            "  BERT+CNN_30_Words F1 score: 0.7683419988445986\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNN_30_Words Accuracy: 15.247349823321555\n",
            "  BERT+CNN_30_Words Training loss: 0.3651944385189892\n",
            "\n",
            "Validation...\n",
            "  BERT+CNN_30_Words Accuracy: 10.352112676056338\n",
            "  BERT+CNN_30_Words Validation loss: 0.6761203181575721\n",
            "  BERT+CNN_30_Words This epoch took: 0:02:22\n",
            "\n",
            "  BERT+CNN_30_Words roc_auc score:  0.5749464668094219\n",
            "  BERT+CNN_30_Words F1 score: 0.7701215981470758\n",
            "===\n",
            "Summary\n",
            "BERT+CNN_30_Words Total time 0:09:29 (h:mm:ss)\n",
            "BERT+CNN_30_Words best acc: 0.6492932862190812\n"
          ]
        }
      ],
      "source": [
        "##=====================================================================================\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model1.train()\n",
        "    for step, batch in enumerate(train_dataloader3):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model1.zero_grad()\n",
        "        out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader3)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader3)\n",
        "    print(\"  BERT+CNN_30_Words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+CNN_30_Words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model1.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model1(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader3)\n",
        "    print(\"  BERT+CNN_30_Words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader3)\n",
        "    print(\"  BERT+CNN_30_Words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNN_30_Words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNN_30_Words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNN_30_Words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model1\n",
        "\n",
        "cnn30_fpr, cnn30_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNN_30_Words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNN_30_Words best acc:', accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-50j_7wR05NL"
      },
      "source": [
        "#BERT+Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB7BukN703ah"
      },
      "outputs": [],
      "source": [
        "class BertLstmClassifier(nn.Module):\n",
        "    def __init__(self, model_tune):\n",
        "        super().__init__()\n",
        "        self.bert = model_tune.bert\n",
        "        self.lstm = nn.LSTM(input_size = 768,\n",
        "                            hidden_size = 768,\n",
        "                            num_layers = 1,\n",
        "                            batch_first = True,\n",
        "                            bidirectional = True)\n",
        "        self.classifier = nn.Linear(768 * 2, 2)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids)\n",
        "        out, _ = self.lstm(bert_output[0])\n",
        "        logits = self.classifier(out[:, 1, :])\n",
        "        return self.softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5I-0wE81Iqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a30007-672a-4585-d249-5f6e45f796d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initializing model\n",
        "model2 = BertCNNClassifier(tuned_model=tuned_model)\n",
        "model2.to(device)\n",
        "# set parameters\n",
        "epochs = 4\n",
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model2.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bl9mJDH22NT"
      },
      "source": [
        "BERT+BiLSTM with Ten_words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Ds82GG1dfx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d972b1ac-20f5-46bc-df19-26429d8c4f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_10_words Accuracy: 13.87279151943463\n",
            "  BERT+BiLSTM_10_words Training loss: 0.4508400951173196\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_10_words Accuracy: 14.95774647887324\n",
            "  BERT+BiLSTM_10_words Validation loss: 0.3804571141659374\n",
            "  BERT+BiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_10_words roc_auc score:  0.9339794883353996\n",
            "  BERT+BiLSTM_10_words F1 score: 0.9479166666666666\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_10_words Accuracy: 14.770318021201414\n",
            "  BERT+BiLSTM_10_words Training loss: 0.39258754895769665\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_10_words Accuracy: 15.126760563380282\n",
            "  BERT+BiLSTM_10_words Validation loss: 0.3692583979015619\n",
            "  BERT+BiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_10_words roc_auc score:  0.9490589428603627\n",
            "  BERT+BiLSTM_10_words F1 score: 0.9559939301972685\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_10_words Accuracy: 15.201413427561837\n",
            "  BERT+BiLSTM_10_words Training loss: 0.36293202364823846\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_10_words Accuracy: 15.0\n",
            "  BERT+BiLSTM_10_words Validation loss: 0.37463315230020333\n",
            "  BERT+BiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_10_words roc_auc score:  0.9308157975237881\n",
            "  BERT+BiLSTM_10_words F1 score: 0.9514844315713251\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_10_words Accuracy: 14.978798586572438\n",
            "  BERT+BiLSTM_10_words Training loss: 0.3766689878681102\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_10_words Accuracy: 14.366197183098592\n",
            "  BERT+BiLSTM_10_words Validation loss: 0.4133131739119409\n",
            "  BERT+BiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_10_words roc_auc score:  0.8813607895541853\n",
            "  BERT+BiLSTM_10_words F1 score: 0.9218967921896792\n",
            "===\n",
            "Summary\n",
            "BERT+BiLSTM_10_words Total time 0:09:20 (h:mm:ss)\n",
            "BERT+BiLSTM_10_words best acc: 0.901060070671378\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model2.train()\n",
        "    for step, batch in enumerate(train_dataloader1):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model2.zero_grad()\n",
        "        out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader1)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader1)\n",
        "    print(\"  BERT+BiLSTM_10_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+BiLSTM_10_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model2.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader1)\n",
        "    print(\"  BERT+BiLSTM_10_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader1)\n",
        "    print(\"  BERT+BiLSTM_10_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+BiLSTM_10_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+BiLSTM_10_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+BiLSTM_10_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model2\n",
        "\n",
        "bilstm10_fpr, bilstm10_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+BiLSTM_10_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+BiLSTM_10_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ICz_4k3cbT"
      },
      "source": [
        "BERT+BiLSTM with Twenty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaV9sxGU3h-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500fd470-b127-4436-c744-1d415e2ec838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_20_words Accuracy: 13.240282685512367\n",
            "  BERT+BiLSTM_20_words Training loss: 0.4606083333492279\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_20_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_20_words Validation loss: 0.6604769729392629\n",
            "  BERT+BiLSTM_20_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_20_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_20_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_20_words Accuracy: 13.480565371024735\n",
            "  BERT+BiLSTM_20_words Training loss: 0.4260601065394735\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_20_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_20_words Validation loss: 0.6416627189642946\n",
            "  BERT+BiLSTM_20_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_20_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_20_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_20_words Accuracy: 13.480565371024735\n",
            "  BERT+BiLSTM_20_words Training loss: 0.42552852735923796\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_20_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_20_words Validation loss: 0.6418259135434325\n",
            "  BERT+BiLSTM_20_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_20_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_20_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_20_words Accuracy: 13.480565371024735\n",
            "  BERT+BiLSTM_20_words Training loss: 0.4255118786024963\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_20_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_20_words Validation loss: 0.6083956192916548\n",
            "  BERT+BiLSTM_20_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+BiLSTM_20_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_20_words F1 score: 0.7401224262659989\n",
            "===\n",
            "Summary\n",
            "BERT+BiLSTM_20_words Total time 0:09:18 (h:mm:ss)\n",
            "BERT+BiLSTM_20_words best acc: 0.5874558303886925\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model2.train()\n",
        "    for step, batch in enumerate(train_dataloader2):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model2.zero_grad()\n",
        "        out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader2)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader2)\n",
        "    print(\"  BERT+BiLSTM_20_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+BiLSTM_20_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model2.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader2)\n",
        "    print(\"  BERT+BiLSTM_20_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader2)\n",
        "    print(\"  BERT+BiLSTM_20_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+BiLSTM_20_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+BiLSTM_20_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+BiLSTM_20_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model2\n",
        "\n",
        "bilstm20_fpr, bilstm20_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+BiLSTM_20_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+BiLSTM_20_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izs9rh7X3yW0"
      },
      "source": [
        "BERT+BiLSTM with Thirty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAd4TIVg34l8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a494d6df-24d2-42ec-f294-f586490121ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_30_words Accuracy: 14.63250883392226\n",
            "  BERT+BiLSTM_30_words Training loss: 0.39701793608732866\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_30_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_30_words Validation loss: 0.6088920172671197\n",
            "  BERT+BiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_30_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_30_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_30_words Accuracy: 14.63250883392226\n",
            "  BERT+BiLSTM_30_words Training loss: 0.3948601106240977\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_30_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_30_words Validation loss: 0.6093385618337444\n",
            "  BERT+BiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_30_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_30_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_30_words Accuracy: 14.63250883392226\n",
            "  BERT+BiLSTM_30_words Training loss: 0.3954830251818411\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_30_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_30_words Validation loss: 0.6103031240718465\n",
            "  BERT+BiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_30_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_30_words F1 score: 0.7401224262659989\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+BiLSTM_30_words Accuracy: 14.63250883392226\n",
            "  BERT+BiLSTM_30_words Training loss: 0.39039900740970573\n",
            "\n",
            "Validation...\n",
            "  BERT+BiLSTM_30_words Accuracy: 9.366197183098592\n",
            "  BERT+BiLSTM_30_words Validation loss: 0.6623733505396776\n",
            "  BERT+BiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+BiLSTM_30_words roc_auc score:  0.5\n",
            "  BERT+BiLSTM_30_words F1 score: 0.7401224262659989\n",
            "===\n",
            "Summary\n",
            "BERT+BiLSTM_30_words Total time 0:09:16 (h:mm:ss)\n",
            "BERT+BiLSTM_30_words best acc: 0.5874558303886925\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model2.train()\n",
        "    for step, batch in enumerate(train_dataloader3):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model2.zero_grad()\n",
        "        out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader3)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader3)\n",
        "    print(\"  BERT+BiLSTM_30_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+BiLSTM_30_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model2.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model2(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader3)\n",
        "    print(\"  BERT+BiLSTM_30_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader3)\n",
        "    print(\"  BERT+BiLSTM_30_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+BiLSTM_30_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+BiLSTM_30_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+BiLSTM_30_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model2\n",
        "\n",
        "bilstm30_fpr, bilstm30_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+BiLSTM_30_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+BiLSTM_30_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws02Obyp_Wvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d69f28e-1b53-4116-8aff-23ab149526c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class BertCNNBiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, tuned_model, embed_num = 512, embed_dim = 768, dropout=0.1, kernel_num=3, kernel_sizes=[1,2], num_labels=2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.embed_num = embed_num\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.softmax = nn.functional.softmax\n",
        "\n",
        "        self.bert = tuned_model.bert\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(1, self.kernel_num, (k, self.embed_dim)) for k in self.kernel_sizes])\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        self.lstms = nn.LSTM(input_size = 768, hidden_size = 768, num_layers = 1, batch_first = True, bidirectional = True)\n",
        "        self.classifier = nn.Linear(len(self.kernel_sizes)*self.kernel_num, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids = None):\n",
        "        output = self.bert(input_ids = input_ids, attention_mask = attention_mask, token_type_ids = token_type_ids) #16,512,768\n",
        "        output = output[0].unsqueeze(1) #16,1,512,768\n",
        "        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs] #16,3,508,1 => #16,3,508\n",
        "        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output] #=> 16,3\n",
        "        output = torch.cat(output, 1)\n",
        "        output = self.dropout(output)\n",
        "        logits = self.classifier(output)\n",
        "        return self.softmax(logits, 1)\n",
        "\n",
        "\n",
        "\n",
        "# Initializing model\n",
        "model3 = BertCNNBiLSTMClassifier(tuned_model=tuned_model)\n",
        "model3.to(device)\n",
        "# set parameters\n",
        "epochs = 4\n",
        "learning_rate = 5e-5\n",
        "optimizer = AdamW(model3.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX7txFtnI5X7"
      },
      "source": [
        "BERT+CNNBiLSTM with Ten_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCrSUUzrBlG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd0b012-ea5f-44b8-9da5-a487e78c3d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.024734982332156\n",
            "  BERT+CNNBiLSTM_10_words Training loss: 0.4747642395563766\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.52112676056338\n",
            "  BERT+CNNBiLSTM_10_words Validation loss: 0.40168526642759084\n",
            "  BERT+CNNBiLSTM_10_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_10_words roc_auc score:  0.918322036354269\n",
            "  BERT+CNNBiLSTM_10_words F1 score: 0.9201581027667983\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.791519434628976\n",
            "  BERT+CNNBiLSTM_10_words Training loss: 0.3947885388199095\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.690140845070422\n",
            "  BERT+CNNBiLSTM_10_words Validation loss: 0.3911288191734905\n",
            "  BERT+CNNBiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNNBiLSTM_10_words roc_auc score:  0.9216064787235756\n",
            "  BERT+CNNBiLSTM_10_words F1 score: 0.9322162985529322\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.321554770318022\n",
            "  BERT+CNNBiLSTM_10_words Training loss: 0.4184641327327216\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 13.704225352112676\n",
            "  BERT+CNNBiLSTM_10_words Validation loss: 0.4557203864547568\n",
            "  BERT+CNNBiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNNBiLSTM_10_words roc_auc score:  0.8332710791969217\n",
            "  BERT+CNNBiLSTM_10_words F1 score: 0.8916155419222903\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.374558303886927\n",
            "  BERT+CNNBiLSTM_10_words Training loss: 0.41483891557888936\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_10_words Accuracy: 14.450704225352112\n",
            "  BERT+CNNBiLSTM_10_words Validation loss: 0.40599075883207186\n",
            "  BERT+CNNBiLSTM_10_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNNBiLSTM_10_words roc_auc score:  0.8998985686915362\n",
            "  BERT+CNNBiLSTM_10_words F1 score: 0.9215976331360948\n",
            "===\n",
            "Summary\n",
            "BERT+CNNBiLSTM_10_words Total time 0:09:19 (h:mm:ss)\n",
            "BERT+CNNBiLSTM_10_words best acc: 0.9063604240282686\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model3.train()\n",
        "    for step, batch in enumerate(train_dataloader1):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model3.zero_grad()\n",
        "        out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader1)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader1)\n",
        "    print(\"  BERT+CNNBiLSTM_10_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+CNNBiLSTM_10_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model3.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader1)\n",
        "    print(\"  BERT+CNNBiLSTM_10_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader1)\n",
        "    print(\"  BERT+CNNBiLSTM_10_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNNBiLSTM_10_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNNBiLSTM_10_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNNBiLSTM_10_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model3\n",
        "\n",
        "cnn_bilstm10_fpr, cnn_bilstm10_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNNBiLSTM_10_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNNBiLSTM_10_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmahMlH8JD8s"
      },
      "source": [
        "BERT+CNNBiLSTM with Twenty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrRxmIfPH86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b323b7-f7cb-4eb0-e7b9-abf3b30837b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 12.989399293286219\n",
            "  BERT+CNNBiLSTM_20_words Training loss: 0.48285040912274335\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 9.014084507042254\n",
            "  BERT+CNNBiLSTM_20_words Validation loss: 0.7243317907125177\n",
            "  BERT+CNNBiLSTM_20_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNNBiLSTM_20_words roc_auc score:  0.48152179163111203\n",
            "  BERT+CNNBiLSTM_20_words F1 score: 0.7220338983050847\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 13.360424028268552\n",
            "  BERT+CNNBiLSTM_20_words Training loss: 0.4718998539995389\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 9.380281690140846\n",
            "  BERT+CNNBiLSTM_20_words Validation loss: 0.6855533719062805\n",
            "  BERT+CNNBiLSTM_20_words This epoch took: 0:02:20\n",
            "\n",
            "  BERT+CNNBiLSTM_20_words roc_auc score:  0.5010706638115632\n",
            "  BERT+CNNBiLSTM_20_words F1 score: 0.7405345211581292\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 13.318021201413428\n",
            "  BERT+CNNBiLSTM_20_words Training loss: 0.4557072323329036\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 9.338028169014084\n",
            "  BERT+CNNBiLSTM_20_words Validation loss: 0.5819648852650549\n",
            "  BERT+CNNBiLSTM_20_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_20_words roc_auc score:  0.49881502471381883\n",
            "  BERT+CNNBiLSTM_20_words F1 score: 0.7384272169548244\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 13.416961130742049\n",
            "  BERT+CNNBiLSTM_20_words Training loss: 0.4463862115418532\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_20_words Accuracy: 12.788732394366198\n",
            "  BERT+CNNBiLSTM_20_words Validation loss: 0.5868509151566197\n",
            "  BERT+CNNBiLSTM_20_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_20_words roc_auc score:  0.7614464426591103\n",
            "  BERT+CNNBiLSTM_20_words F1 score: 0.8551099611901681\n",
            "===\n",
            "Summary\n",
            "BERT+CNNBiLSTM_20_words Total time 0:09:18 (h:mm:ss)\n",
            "BERT+CNNBiLSTM_20_words best acc: 0.8021201413427562\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model3.train()\n",
        "    for step, batch in enumerate(train_dataloader2):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model3.zero_grad()\n",
        "        out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader2)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader2)\n",
        "    print(\"  BERT+CNNBiLSTM_20_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+CNNBiLSTM_20_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model3.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader1:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader2)\n",
        "    print(\"  BERT+CNNBiLSTM_20_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader2)\n",
        "    print(\"  BERT+CNNBiLSTM_20_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNNBiLSTM_20_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNNBiLSTM_20_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNNBiLSTM_20_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model3\n",
        "\n",
        "cnn_bilstm20_fpr, cnn_bilstm20_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNNBiLSTM_20_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNNBiLSTM_20_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0klv81X4JH1s"
      },
      "source": [
        "BERT+CNNBiLSTM with Thirty_Words Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbPHu078IYmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce76b416-0700-4b61-ef38-3ee100ba327d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.869257950530036\n",
            "  BERT+CNNBiLSTM_30_words Training loss: 0.40257470967912845\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.95774647887324\n",
            "  BERT+CNNBiLSTM_30_words Validation loss: 0.4054386254767297\n",
            "  BERT+CNNBiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_30_words roc_auc score:  0.8127914752446872\n",
            "  BERT+CNNBiLSTM_30_words F1 score: 0.9279509453244761\n",
            "\n",
            "Epoch 2 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 14.017667844522968\n",
            "  BERT+CNNBiLSTM_30_words Training loss: 0.39920514685105096\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.126760563380282\n",
            "  BERT+CNNBiLSTM_30_words Validation loss: 0.42456614845235585\n",
            "  BERT+CNNBiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_30_words roc_auc score:  0.8074461340720991\n",
            "  BERT+CNNBiLSTM_30_words F1 score: 0.8939554612937434\n",
            "\n",
            "Epoch 3 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.293286219081272\n",
            "  BERT+CNNBiLSTM_30_words Training loss: 0.4154240547978836\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.295774647887324\n",
            "  BERT+CNNBiLSTM_30_words Validation loss: 0.42174015708372625\n",
            "  BERT+CNNBiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_30_words roc_auc score:  0.8094001580588291\n",
            "  BERT+CNNBiLSTM_30_words F1 score: 0.9010526315789473\n",
            "\n",
            "Epoch 4 / 4\n",
            "Training...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.33922261484099\n",
            "  BERT+CNNBiLSTM_30_words Training loss: 0.4138495369218685\n",
            "\n",
            "Validation...\n",
            "  BERT+CNNBiLSTM_30_words Accuracy: 13.126760563380282\n",
            "  BERT+CNNBiLSTM_30_words Validation loss: 0.42326282279592164\n",
            "  BERT+CNNBiLSTM_30_words This epoch took: 0:02:19\n",
            "\n",
            "  BERT+CNNBiLSTM_30_words roc_auc score:  0.815314337325332\n",
            "  BERT+CNNBiLSTM_30_words F1 score: 0.8937300743889478\n",
            "===\n",
            "Summary\n",
            "BERT+CNNBiLSTM_30_words Total time 0:09:16 (h:mm:ss)\n",
            "BERT+CNNBiLSTM_30_words best acc: 0.823321554770318\n"
          ]
        }
      ],
      "source": [
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model3.train()\n",
        "    for step, batch in enumerate(train_dataloader3):\n",
        "\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model3.zero_grad()\n",
        "        out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_train_accuracy +=  torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader3)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader3)\n",
        "    print(\"  BERT+CNNBiLSTM_30_words Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"  BERT+CNNBiLSTM_30_words Training loss: {}\".format(avg_train_loss))\n",
        "\n",
        "    # Validation\n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        "    model3.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader3:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model3(input_ids = input_ids, attention_mask = input_mask, token_type_ids = None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader3)\n",
        "    print(\"  BERT+CNNBiLSTM_30_words Accuracy: {}\".format(avg_val_accuracy))\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader3)\n",
        "    print(\"  BERT+CNNBiLSTM_30_words Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  BERT+CNNBiLSTM_30_words This epoch took: {:}\".format(training_time))\n",
        "    print()\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('  BERT+CNNBiLSTM_30_words roc_auc score: ', roc_auc_score(y_true,y_pred))\n",
        "    print('  BERT+CNNBiLSTM_30_words F1 score:',f1_score(y_true, y_pred))\n",
        "\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model3\n",
        "\n",
        "cnn_bilstm30_fpr, cnn_bilstm30_tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "\n",
        "print(\"===\")\n",
        "print(\"Summary\")\n",
        "print(\"BERT+CNNBiLSTM_30_words Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('BERT+CNNBiLSTM_30_words best acc:',accuracy_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roc_curve_data = {\n",
        "     'Name': [\"cnn10_fpr\", \"cnn10_tpr\", \"cnn20_fpr\", \"cnn20_tpr\", \"cnn30_fpr\", \"cnn30_tpr\", \"bilstm10_fpr\", \"bilstm10_tpr\", \"bilstm20_fpr\", \"bilstm20_tpr\", \"bilstm30_fpr\", \"bilstm30_tpr\", \"cnn_bilstm10_fpr\", \"cnn_bilstm10_tpr\", \"cnn_bilstm20_fpr\", \"cnn_bilstm20_tpr\", \"cnn_bilstm30_fpr\", \"cnn_bilstm30_tpr\"],\n",
        "     'Scores': [cnn10_fpr, cnn10_tpr, cnn20_fpr, cnn20_tpr, cnn30_fpr, cnn30_tpr, bilstm10_fpr, bilstm10_tpr, bilstm20_fpr, bilstm20_tpr, bilstm30_fpr, bilstm30_tpr, cnn_bilstm10_fpr, cnn_bilstm10_tpr, cnn_bilstm20_fpr, cnn_bilstm20_tpr, cnn_bilstm30_fpr, cnn_bilstm30_tpr,]\n",
        "}\n",
        "df = pd.DataFrame(roc_curve_data).transpose()\n",
        "df.to_excel('BERT_ROC_Curve_Scores_file.xlsx', index=False)\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "lK_odqLaNCXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1897f927-0963-416b-9884-cf105b24edf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     0                              1   \\\n",
            "Name                          cnn10_fpr                      cnn10_tpr   \n",
            "Scores  [0.0, 0.06209850107066381, 1.0]  [0.0, 0.968421052631579, 1.0]   \n",
            "\n",
            "                                    2                3   \\\n",
            "Name                         cnn20_fpr        cnn20_tpr   \n",
            "Scores  [0.0, 0.7366167023554604, 1.0]  [0.0, 1.0, 1.0]   \n",
            "\n",
            "                                    4                5   \\\n",
            "Name                         cnn30_fpr        cnn30_tpr   \n",
            "Scores  [0.0, 0.8501070663811563, 1.0]  [0.0, 1.0, 1.0]   \n",
            "\n",
            "                                     6                               7   \\\n",
            "Name                       bilstm10_fpr                    bilstm10_tpr   \n",
            "Scores  [0.0, 0.23126338329764454, 1.0]  [0.0, 0.9939849624060151, 1.0]   \n",
            "\n",
            "                  8             9             10            11  \\\n",
            "Name    bilstm20_fpr  bilstm20_tpr  bilstm30_fpr  bilstm30_tpr   \n",
            "Scores    [0.0, 1.0]    [0.0, 1.0]    [0.0, 1.0]    [0.0, 1.0]   \n",
            "\n",
            "                                     12                              13  \\\n",
            "Name                   cnn_bilstm10_fpr                cnn_bilstm10_tpr   \n",
            "Scores  [0.0, 0.13704496788008566, 1.0]  [0.0, 0.9368421052631579, 1.0]   \n",
            "\n",
            "                                     14                              15  \\\n",
            "Name                   cnn_bilstm20_fpr                cnn_bilstm20_tpr   \n",
            "Scores  [0.0, 0.47109207708779444, 1.0]  [0.0, 0.9939849624060151, 1.0]   \n",
            "\n",
            "                                     16                              17  \n",
            "Name                   cnn_bilstm30_fpr                cnn_bilstm30_tpr  \n",
            "Scores  [0.0, 0.19469026548672566, 1.0]  [0.0, 0.8253189401373896, 1.0]  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyON/SYF6ZRhmjnSqU9qaNWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}